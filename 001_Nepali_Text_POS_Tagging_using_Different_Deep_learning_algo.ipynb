{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6V26rHXmdfyHSXxphA346",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khojwar/Master_Thesis/blob/main/001_Nepali_Text_POS_Tagging_using_Different_Deep_learning_algo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part of speech tagging using rnn model"
      ],
      "metadata": {
        "id": "fNDfbXAyD_EE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-speech (POS) tagging is a fundamental natural language processing task that involves assigning a grammatical category (such as noun, verb, adjective, etc.) to each word in a sentence. Recurrent Neural Networks (RNNs) can be used for POS tagging, although more advanced models like Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU) are often preferred due to their ability to capture longer-range dependencies.\n",
        "\n",
        "Here's a step-by-step guide to building a basic POS tagging model using an RNN (specifically, an LSTM) in Python with the help of libraries like TensorFlow and Keras:\n",
        "\n",
        "1. Data Preparation:\n",
        "Prepare your training data in the form of sentences with corresponding POS tags. You can use datasets like Penn Treebank or Universal Dependencies for this purpose.\n",
        "\n",
        "2. Tokenization and Padding:\n",
        "Convert your sentences into sequences of word tokens, and then pad the sequences to ensure uniform length.\n",
        "\n",
        "3. Data Vectorization:\n",
        "Convert the word tokens and POS tags into numerical vectors using techniques like one-hot encoding or word embeddings.\n",
        "\n",
        "4. Model Architecture:\n",
        "Build your RNN model using an LSTM layer. You can experiment with stacking multiple LSTM layers or combining them with other types of layers like Dense layers.\n",
        "\n",
        "5. Compile the Model:\n",
        "Compile your model using an appropriate loss function (categorical cross-entropy) and optimizer (e.g., Adam).\n",
        "\n",
        "6. Model Training:\n",
        "Train your model on the prepared training data. Monitor the training process and adjust hyperparameters as needed.\n",
        "\n",
        "7. Model Evaluation:\n",
        "Evaluate your model's performance on a separate validation or test dataset. Calculate metrics like accuracy, precision, recall, and F1-score.\n",
        "\n",
        "8. Inference:\n",
        "Use your trained model for POS tagging by feeding it new sentences. The model will output the predicted POS tags for each word.\n",
        "\n",
        "Here's a code snippet illustrating the implementation using Keras:"
      ],
      "metadata": {
        "id": "qcytZvL5F97l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "rSSwAFWfD6_Q",
        "outputId": "4e50efdc-36d5-4a4a-d517-537b468afaab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 5s 5s/step - loss: 1.6071 - accuracy: 0.5000 - val_loss: 1.6093 - val_accuracy: 0.1667\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6015 - accuracy: 0.5000 - val_loss: 1.6095 - val_accuracy: 0.1667\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5959 - accuracy: 0.3333 - val_loss: 1.6095 - val_accuracy: 0.1667\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.5902 - accuracy: 0.3333 - val_loss: 1.6096 - val_accuracy: 0.1667\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.5843 - accuracy: 0.3333 - val_loss: 1.6095 - val_accuracy: 0.1667\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.5782 - accuracy: 0.3333 - val_loss: 1.6094 - val_accuracy: 0.1667\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.5718 - accuracy: 0.3333 - val_loss: 1.6093 - val_accuracy: 0.1667\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.5651 - accuracy: 0.3333 - val_loss: 1.6090 - val_accuracy: 0.1667\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.5580 - accuracy: 0.3333 - val_loss: 1.6087 - val_accuracy: 0.1667\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.5503 - accuracy: 0.3333 - val_loss: 1.6084 - val_accuracy: 0.1667\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ff54f78627d5>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'The'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'barked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mtest_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpredicted_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ff54f78627d5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'The'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'barked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mtest_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpredicted_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'dog'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Example data (replace with your own)\n",
        "sentences = [['The', 'cat', 'is', 'on', 'the', 'mat'],\n",
        "             ['I', 'ate', 'an', 'apple']]\n",
        "\n",
        "pos_tags = [['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN'],\n",
        "            ['PRON', 'VERB', 'DET', 'NOUN']]\n",
        "\n",
        "# Create vocabulary and tag sets\n",
        "word_vocab = set(word for sent in sentences for word in sent)\n",
        "tag_vocab = set(tag for tag_seq in pos_tags for tag in tag_seq)\n",
        "\n",
        "word_to_index = {word: idx + 1 for idx, word in enumerate(word_vocab)}\n",
        "tag_to_index = {tag: idx for idx, tag in enumerate(tag_vocab)}\n",
        "\n",
        "# Convert words and tags to numerical values\n",
        "X = [[word_to_index[word] for word in sent] for sent in sentences]\n",
        "y = [[tag_to_index[tag] for tag in tag_seq] for tag_seq in pos_tags]\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = max(len(seq) for seq in X)\n",
        "X_padded = pad_sequences(X, maxlen=max_sequence_length, padding='post')\n",
        "y_padded = pad_sequences(y, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convert tags to one-hot encoding\n",
        "num_tags = len(tag_vocab)\n",
        "y_onehot = np.array([to_categorical(tag_seq, num_classes=num_tags) for tag_seq in y_padded])\n",
        "\n",
        "# Build and compile the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_vocab) + 1, output_dim=50, input_length=max_sequence_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dense(num_tags, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_padded, y_onehot, batch_size=32, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Inference\n",
        "test_sentence = ['The', 'dog', 'barked']\n",
        "test_sequence = [word_to_index[word] for word in test_sentence]\n",
        "test_padded = pad_sequences([test_sequence], maxlen=max_sequence_length, padding='post')\n",
        "predicted_probs = model.predict(test_padded)\n",
        "predicted_tags = [tag_vocab[np.argmax(tag_prob)] for tag_prob in predicted_probs[0]]\n",
        "\n",
        "print(predicted_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nepali text pos tagging using gru algorithms"
      ],
      "metadata": {
        "id": "9bypN2OhGvam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, I can help you with that. Here's a step-by-step guide to building a Nepali text POS tagging model using the GRU (Gated Recurrent Unit) algorithm with Python and libraries like TensorFlow and Keras. Please note that you'll need to prepare or obtain a Nepali POS tagged dataset for training.\n",
        "\n",
        "1. Data Preparation:\n",
        "Prepare your Nepali POS tagged dataset. Each sentence should be tokenized into words and accompanied by their corresponding POS tags.\n",
        "\n",
        "2. Tokenization and Padding:\n",
        "Convert the Nepali sentences into sequences of word tokens, and then pad the sequences to ensure uniform length.\n",
        "\n",
        "3. Data Vectorization:\n",
        "Convert the word tokens and POS tags into numerical vectors using techniques like one-hot encoding or word embeddings.\n",
        "\n",
        "4. Model Architecture:\n",
        "Build your GRU model using the Keras library. You can experiment with different configurations and hyperparameters.\n",
        "\n",
        "5. Compile the Model:\n",
        "Compile your model using an appropriate loss function (categorical cross-entropy) and optimizer (e.g., Adam).\n",
        "\n",
        "6. Model Training:\n",
        "Train your model on the prepared training data. Monitor the training process and adjust hyperparameters as needed.\n",
        "\n",
        "7. Model Evaluation:\n",
        "Evaluate your model's performance on a separate validation or test dataset. Calculate metrics like accuracy, precision, recall, and F1-score.\n",
        "\n",
        "8. Inference:\n",
        "Use your trained model for POS tagging by feeding it new Nepali sentences. The model will output the predicted POS tags for each word.\n",
        "\n",
        "Here's a simplified code snippet illustrating the implementation using Keras:"
      ],
      "metadata": {
        "id": "Oxzb6UGrG0Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Example data (replace with your own)\n",
        "nepali_sentences = [['नेपाल', 'बाट', 'सुन्दर', 'माउन्ट', 'एभरेस्ट', 'देखिन्छ'],\n",
        "                    ['म', 'काठमाडौं', 'जान्न', 'गइन्']]\n",
        "\n",
        "nepali_pos_tags = [['NOUN', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'VERB'],\n",
        "                   ['PRON', 'NOUN', 'VERB', 'VERB']]\n",
        "\n",
        "# Create vocabulary and tag sets\n",
        "word_vocab = set(word for sent in nepali_sentences for word in sent)\n",
        "tag_vocab = set(tag for tag_seq in nepali_pos_tags for tag in tag_seq)\n",
        "\n",
        "word_to_index = {word: idx + 1 for idx, word in enumerate(word_vocab)}\n",
        "tag_to_index = {tag: idx for idx, tag in enumerate(tag_vocab)}\n",
        "\n",
        "# Convert words and tags to numerical values\n",
        "X = [[word_to_index[word] for word in sent] for sent in nepali_sentences]\n",
        "y = [[tag_to_index[tag] for tag in tag_seq] for tag_seq in nepali_pos_tags]\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = max(len(seq) for seq in X)\n",
        "X_padded = pad_sequences(X, maxlen=max_sequence_length, padding='post')\n",
        "y_padded = pad_sequences(y, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convert tags to one-hot encoding\n",
        "num_tags = len(tag_vocab)\n",
        "y_onehot = np.array([to_categorical(tag_seq, num_classes=num_tags) for tag_seq in y_padded])\n",
        "\n",
        "# Build and compile the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_vocab) + 1, output_dim=50, input_length=max_sequence_length))\n",
        "model.add(GRU(100, return_sequences=True))\n",
        "model.add(Dense(num_tags, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_padded, y_onehot, batch_size=32, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Inference\n",
        "test_sentence = ['नेपाल', 'मा', 'राम्रो', 'ठाउँ', 'छ']\n",
        "test_sequence = [word_to_index[word] for word in test_sentence]\n",
        "test_padded = pad_sequences([test_sequence], maxlen=max_sequence_length, padding='post')\n",
        "predicted_probs = model.predict(test_padded)\n",
        "predicted_tags = [tag_vocab[np.argmax(tag_prob)] for tag_prob in predicted_probs[0]]\n",
        "\n",
        "print(predicted_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "fdYh8E2mFyrd",
        "outputId": "40727098-d347-4113-b22b-c1047f0472af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 5s 5s/step - loss: 1.6091 - accuracy: 0.1667 - val_loss: 1.6052 - val_accuracy: 0.1667\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5961 - accuracy: 0.5000 - val_loss: 1.6056 - val_accuracy: 0.1667\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5831 - accuracy: 0.5000 - val_loss: 1.6060 - val_accuracy: 0.1667\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 1.5701 - accuracy: 0.5000 - val_loss: 1.6063 - val_accuracy: 0.1667\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.5569 - accuracy: 0.5000 - val_loss: 1.6065 - val_accuracy: 0.1667\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.5433 - accuracy: 0.5000 - val_loss: 1.6067 - val_accuracy: 0.1667\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.5294 - accuracy: 0.5000 - val_loss: 1.6069 - val_accuracy: 0.1667\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.5149 - accuracy: 0.5000 - val_loss: 1.6072 - val_accuracy: 0.1667\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.4998 - accuracy: 0.5000 - val_loss: 1.6075 - val_accuracy: 0.1667\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.4840 - accuracy: 0.5000 - val_loss: 1.6079 - val_accuracy: 0.1667\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b6fd7e0aeddc>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'नेपाल'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'मा'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'राम्रो'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ठाउँ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'छ'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mtest_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpredicted_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b6fd7e0aeddc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'नेपाल'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'मा'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'राम्रो'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ठाउँ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'छ'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mtest_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpredicted_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'मा'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nepali text pos tagging using lstm algorithms"
      ],
      "metadata": {
        "id": "h0qmDafNHbff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Example data (replace with your own)\n",
        "nepali_sentences = [['नेपाल', 'बाट', 'सुन्दर', 'माउन्ट', 'एभरेस्ट', 'देखिन्छ'],\n",
        "                    ['म', 'काठमाडौं', 'जान्न', 'गइन्']]\n",
        "\n",
        "nepali_pos_tags = [['NOUN', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'VERB'],\n",
        "                   ['PRON', 'NOUN', 'VERB', 'VERB']]\n",
        "\n",
        "# Create vocabulary and tag sets\n",
        "word_vocab = set(word for sent in nepali_sentences for word in sent)\n",
        "tag_vocab = set(tag for tag_seq in nepali_pos_tags for tag in tag_seq)\n",
        "\n",
        "word_to_index = {word: idx + 1 for idx, word in enumerate(word_vocab)}\n",
        "tag_to_index = {tag: idx for idx, tag in enumerate(tag_vocab)}\n",
        "\n",
        "# Convert words and tags to numerical values\n",
        "X = [[word_to_index[word] for word in sent] for sent in nepali_sentences]\n",
        "y = [[tag_to_index[tag] for tag in tag_seq] for tag_seq in nepali_pos_tags]\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = max(len(seq) for seq in X)\n",
        "X_padded = pad_sequences(X, maxlen=max_sequence_length, padding='post')\n",
        "y_padded = pad_sequences(y, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convert tags to one-hot encoding\n",
        "num_tags = len(tag_vocab)\n",
        "y_onehot = np.array([to_categorical(tag_seq, num_classes=num_tags) for tag_seq in y_padded])\n",
        "\n",
        "# Build and compile the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_vocab) + 1, output_dim=50, input_length=max_sequence_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dense(num_tags, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_padded, y_onehot, batch_size=32, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Inference\n",
        "test_sentence = ['नेपाल', 'मा', 'राम्रो', 'ठाउँ', 'छ']\n",
        "test_sequence = [word_to_index[word] for word in test_sentence]\n",
        "test_padded = pad_sequences([test_sequence], maxlen=max_sequence_length, padding='post')\n",
        "predicted_probs = model.predict(test_padded)\n",
        "predicted_tags = [tag_vocab[np.argmax(tag_prob)] for tag_prob in predicted_probs[0]]\n",
        "\n",
        "print(predicted_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "kWKaROxeHB_k",
        "outputId": "9957006f-704d-45f3-9872-dcb911c6b783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.6102 - accuracy: 0.1667 - val_loss: 1.6083 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.6039 - accuracy: 0.3333 - val_loss: 1.6086 - val_accuracy: 0.1667\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.5975 - accuracy: 0.5000 - val_loss: 1.6090 - val_accuracy: 0.1667\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.5911 - accuracy: 0.5000 - val_loss: 1.6093 - val_accuracy: 0.1667\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.5844 - accuracy: 0.5000 - val_loss: 1.6098 - val_accuracy: 0.1667\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.5775 - accuracy: 0.5000 - val_loss: 1.6103 - val_accuracy: 0.1667\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5702 - accuracy: 0.5000 - val_loss: 1.6108 - val_accuracy: 0.1667\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.5624 - accuracy: 0.5000 - val_loss: 1.6114 - val_accuracy: 0.1667\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.5541 - accuracy: 0.5000 - val_loss: 1.6120 - val_accuracy: 0.1667\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.5451 - accuracy: 0.5000 - val_loss: 1.6128 - val_accuracy: 0.1667\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-72df04301366>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'नेपाल'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'मा'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'राम्रो'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ठाउँ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'छ'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mtest_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpredicted_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-72df04301366>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'नेपाल'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'मा'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'राम्रो'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ठाउँ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'छ'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mtest_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpredicted_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'मा'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nepali text pos tagging using bi-lstm algorithms"
      ],
      "metadata": {
        "id": "wmFO-YRdH0pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Example data (replace with your own)\n",
        "nepali_sentences = [['नेपाल', 'बाट', 'सुन्दर', 'माउन्ट', 'एभरेस्ट', 'देखिन्छ'],\n",
        "                    ['म', 'काठमाडौं', 'जान्न', 'गइन्']]\n",
        "\n",
        "nepali_pos_tags = [['NOUN', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'VERB'],\n",
        "                   ['PRON', 'NOUN', 'VERB', 'VERB']]\n",
        "\n",
        "# Create vocabulary and tag sets\n",
        "word_vocab = set(word for sent in nepali_sentences for word in sent)\n",
        "tag_vocab = set(tag for tag_seq in nepali_pos_tags for tag in tag_seq)\n",
        "\n",
        "word_to_index = {word: idx + 1 for idx, word in enumerate(word_vocab)}\n",
        "tag_to_index = {tag: idx for idx, tag in enumerate(tag_vocab)}\n",
        "\n",
        "# Convert words and tags to numerical values\n",
        "X = [[word_to_index[word] for word in sent] for sent in nepali_sentences]\n",
        "y = [[tag_to_index[tag] for tag in tag_seq] for tag_seq in nepali_pos_tags]\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = max(len(seq) for seq in X)\n",
        "X_padded = pad_sequences(X, maxlen=max_sequence_length, padding='post')\n",
        "y_padded = pad_sequences(y, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convert tags to one-hot encoding\n",
        "num_tags = len(tag_vocab)\n",
        "y_onehot = np.array([to_categorical(tag_seq, num_classes=num_tags) for tag_seq in y_padded])\n",
        "\n",
        "# Build and compile the Bi-LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_vocab) + 1, output_dim=50, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "model.add(Dense(num_tags, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_padded, y_onehot, batch_size=32, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Inference\n",
        "test_sentence = ['नेपाल', 'मा', 'राम्रो', 'ठाउँ', 'छ']\n",
        "test_sequence = [word_to_index[word] for word in test_sentence]\n",
        "test_padded = pad_sequences([test_sequence], maxlen=max_sequence_length, padding='post')\n",
        "predicted_probs = model.predict(test_padded)\n",
        "predicted_tags = [tag_vocab[np.argmax(tag_prob)] for tag_prob in predicted_probs[0]]\n",
        "\n",
        "print(predicted_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "rWjfhxL3Hf87",
        "outputId": "13b175f2-e41d-4924-f5ae-54cec7e81873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.6080 - accuracy: 0.0000e+00 - val_loss: 1.6023 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.5985 - accuracy: 0.5000 - val_loss: 1.6026 - val_accuracy: 0.1667\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.5890 - accuracy: 0.5000 - val_loss: 1.6029 - val_accuracy: 0.1667\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.5795 - accuracy: 0.5000 - val_loss: 1.6032 - val_accuracy: 0.1667\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.5697 - accuracy: 0.5000 - val_loss: 1.6036 - val_accuracy: 0.1667\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.5595 - accuracy: 0.5000 - val_loss: 1.6040 - val_accuracy: 0.1667\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.5487 - accuracy: 0.5000 - val_loss: 1.6044 - val_accuracy: 0.1667\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.5373 - accuracy: 0.5000 - val_loss: 1.6049 - val_accuracy: 0.1667\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 1.5250 - accuracy: 0.5000 - val_loss: 1.6054 - val_accuracy: 0.1667\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.5117 - accuracy: 0.5000 - val_loss: 1.6060 - val_accuracy: 0.1667\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bbd2c8e6a169>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'नेपाल'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'मा'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'राम्रो'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ठाउँ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'छ'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mtest_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpredicted_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bbd2c8e6a169>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'नेपाल'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'मा'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'राम्रो'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ठाउँ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'छ'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mtest_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpredicted_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'मा'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# nepali text pos tagging using  mbert"
      ],
      "metadata": {
        "id": "T7UjsTGxIEx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using mBERT (Multilingual BERT) for Nepali text POS tagging involves fine-tuning a pre-trained mBERT model on a Nepali POS tagging dataset. Here's a general step-by-step guide to help you get started:\n",
        "\n",
        "1. Data Preparation:\n",
        "Prepare your Nepali POS tagged dataset. Each sentence should be tokenized into words and accompanied by their corresponding POS tags.\n",
        "\n",
        "2. Tokenization and Padding:\n",
        "Convert the Nepali sentences into sequences of word tokens and pad them to ensure uniform length.\n",
        "\n",
        "3. Data Vectorization:\n",
        "Convert the word tokens and POS tags into numerical vectors using techniques like one-hot encoding or word embeddings.\n",
        "\n",
        "4. Preprocessing for mBERT:\n",
        "Tokenize your sentences using a pre-trained mBERT tokenizer designed for Nepali text. You may need to install the transformers library by Hugging Face for this purpose.\n",
        "\n",
        "5. Fine-Tuning mBERT:\n",
        "Load a pre-trained mBERT model and fine-tune it on your Nepali POS tagging dataset. You'll need to create a custom classification layer on top of the mBERT model for POS tagging.\n",
        "\n",
        "6. Compile and Train:\n",
        "Compile your model using an appropriate loss function (categorical cross-entropy) and optimizer. Train the model on the prepared training data.\n",
        "\n",
        "7. Model Evaluation:\n",
        "Evaluate your fine-tuned mBERT model's performance on a separate validation or test dataset. Calculate metrics like accuracy, precision, recall, and F1-score.\n",
        "\n",
        "8. Inference:\n",
        "Use your trained model for POS tagging by feeding it new Nepali sentences. The model will output the predicted POS tags for each word.\n",
        "\n",
        "Here's a simplified code snippet illustrating the implementation using the transformers library and Keras:"
      ],
      "metadata": {
        "id": "dEqFv2wMIbKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Example data (replace with your own)\n",
        "nepali_sentences = [['नेपाल', 'बाट', 'सुन्दर', 'माउन्ट', 'एभरेस्ट', 'देखिन्छ'],\n",
        "                    ['म', 'काठमाडौं', 'जान्न', 'गइन्']]\n",
        "\n",
        "nepali_pos_tags = [['NOUN', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'VERB'],\n",
        "                   ['PRON', 'NOUN', 'VERB', 'VERB']]\n",
        "\n",
        "# Create vocabulary and tag sets\n",
        "word_vocab = set(word for sent in nepali_sentences for word in sent)\n",
        "tag_vocab = set(tag for tag_seq in nepali_pos_tags for tag in tag_seq)\n",
        "\n",
        "word_to_index = {word: idx + 1 for idx, word in enumerate(word_vocab)}\n",
        "tag_to_index = {tag: idx for idx, tag in enumerate(tag_vocab)}\n",
        "\n",
        "# Convert words and tags to numerical values\n",
        "X = [[word_to_index[word] for word in sent] for sent in nepali_sentences]\n",
        "y = [[tag_to_index[tag] for tag in tag_seq] for tag_seq in nepali_pos_tags]\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = max(len(seq) for seq in X)\n",
        "X_padded = pad_sequences(X, maxlen=max_sequence_length, padding='post')\n",
        "y_padded = pad_sequences(y, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convert tags to one-hot encoding\n",
        "num_tags = len(tag_vocab)\n",
        "y_onehot = np.array([to_categorical(tag_seq, num_classes=num_tags) for tag_seq in y_padded])\n",
        "\n",
        "# Load mBERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_inputs = tokenizer(nepali_sentences, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Load mBERT model for sequence classification\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_tags)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(tokenized_inputs, y_onehot, batch_size=32, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Inference\n",
        "test_sentence = ['नेपाल', 'मा', 'राम्रो', 'ठाउँ', 'छ']\n",
        "test_input = tokenizer(test_sentence, padding=True, truncation=True, return_tensors='tf')\n",
        "predicted_probs = model.predict(test_input)\n",
        "predicted_tags = [tag_vocab[np.argmax(tag_prob)] for tag_prob in predicted_probs]\n",
        "\n",
        "print(predicted_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "4rkyARs-H5Zr",
        "outputId": "ce68bf71-35e8-40ec-a0f3-17394eea2c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8bebeed936f5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z39cZJxhIUdy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}