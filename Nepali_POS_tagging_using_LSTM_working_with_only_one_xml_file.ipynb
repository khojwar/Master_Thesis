{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### file path:\n",
        "\n",
        "C:\\Users\\Computer\\Desktop\\nnc_updated_ah\\gc\\books\\book-academic-jana-siksha-2058"
      ],
      "metadata": {
        "id": "qE6wcEAepPgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### how to preprocess this type of data to train postagging model using LSTM algorithms. explain step by step procedure with code"
      ],
      "metadata": {
        "id": "08xh0xULps1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing textual data for training a part-of-speech tagging model using LSTM algorithms involves several steps. I'll provide you with a step-by-step procedure along with Python code snippets for each step. For this example, I'll assume you're using the NLTK library for natural language processing tasks."
      ],
      "metadata": {
        "id": "J2f47yTbp2la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Loading and Parsing:\n",
        "\n",
        "Load the XML data and extract the text content for preprocessing."
      ],
      "metadata": {
        "id": "OFOSNXhup7Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Load the XML data\n",
        "tree = ET.parse('book-academic-jana-siksha-2058.xml')\n",
        "root = tree.getroot()\n",
        "\n",
        "# Extract text content from <w> tags\n",
        "text_data = ' '.join([w.text for w in root.iter('w')])\n",
        "# text_data\n"
      ],
      "metadata": {
        "id": "wgYi03qxeB7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Text Tokenization:\n",
        "Tokenize the text data into words."
      ],
      "metadata": {
        "id": "AAWXf-f3rWj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the text data\n",
        "tokens = word_tokenize(text_data)\n",
        "# tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx2kphgXpO3M",
        "outputId": "563e0367-3fca-4b8c-8312-34f49e18ff22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Remove Special Tags:\n",
        "Remove any special tags like `< cesDoc >`, `< cesHeader >`, etc., and keep only the actual word tokens."
      ],
      "metadata": {
        "id": "SYbtb8rNszME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token for token in tokens if not token.startswith('<')]\n",
        "# tokens"
      ],
      "metadata": {
        "id": "WNt1SBTJrfGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Text Normalization:\n",
        "Normalize the text by converting it to lowercase."
      ],
      "metadata": {
        "id": "SYCmZH8Htft_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_tokens = [token.lower() for token in tokens]\n"
      ],
      "metadata": {
        "id": "ZU2TzjG6tPSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Create Vocabulary and Index Mapping:\n",
        "Create a vocabulary and map words to unique indices."
      ],
      "metadata": {
        "id": "w19nI1tqueRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(normalized_tokens)\n",
        "word_to_index = {word: index + 1 for index, word in enumerate(vocab)}  # Start index from 1\n",
        "index_to_word = {index: word for word, index in word_to_index.items()}\n"
      ],
      "metadata": {
        "id": "U2lQHy50uVLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Convert Words to Indices:\n",
        "Convert tokens to corresponding indices."
      ],
      "metadata": {
        "id": "Id9_DIV_veif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_data = [word_to_index[word] for word in normalized_tokens]\n"
      ],
      "metadata": {
        "id": "wNqZAfAKuYwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Generate Training Examples:\n",
        "Generate input-output pairs for training the LSTM model. The input is a sequence of words, and the output is the corresponding part-of-speech tags."
      ],
      "metadata": {
        "id": "5nshzL0XwBuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Perform part-of-speech tagging on the original text\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Create a mapping from part-of-speech tags to unique indices\n",
        "tag_set = set(tag for word, tag in pos_tags)\n",
        "tag_to_index = {tag: index for index, tag in enumerate(tag_set)}\n",
        "\n",
        "# Generate training examples\n",
        "sequence_length = 10  # You can adjust this value as needed\n",
        "X = [indexed_data[i:i+sequence_length] for i in range(len(indexed_data) - sequence_length)]\n",
        "y = [tag_to_index[pos_tags[i + sequence_length][1]] for i in range(len(indexed_data) - sequence_length)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XUnb4_rv1xU",
        "outputId": "f00e59d3-feb1-4214-a2d5-723825697964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Padding Sequences:\n",
        "Pad sequences to a fixed length for training."
      ],
      "metadata": {
        "id": "SgHjloP81sdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_padded = pad_sequences(X, maxlen=sequence_length, padding='post')\n",
        "X_padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmVO3RT92W5o",
        "outputId": "7802c33f-95f9-4905-9c37-0bbd3f130be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4491, 4548, 5103, ..., 4785, 5698, 3572],\n",
              "       [4548, 5103, 2933, ..., 5698, 3572, 3678],\n",
              "       [5103, 2933, 6642, ..., 3572, 3678, 1165],\n",
              "       ...,\n",
              "       [1454, 2304, 4482, ..., 2180, 4183, 5029],\n",
              "       [2304, 4482, 5426, ..., 4183, 5029, 5016],\n",
              "       [4482, 5426, 5029, ..., 5029, 5016, 2259]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. One-Hot Encoding:\n",
        "Convert part-of-speech tags to one-hot encoded vectors."
      ],
      "metadata": {
        "id": "vxDmLOKN30WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " `np.eye` function is used to create a 2D identity matrix with ones on the diagonal and zeros elsewhere.\n",
        "\n",
        "`len(tag_set)` : Number of rows in the output matrix"
      ],
      "metadata": {
        "id": "4ZMpOaLg6BRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_encoded = np.eye(len(tag_set))[y]\n"
      ],
      "metadata": {
        "id": "9hYWXwu_103G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVB3_Am239mX",
        "outputId": "c8091f08-87a9-4569-f746-e832af7123d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2nd part\n"
      ],
      "metadata": {
        "id": "F0tRkJAm9YAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Train Word2Vec Model:\n",
        "Train a Word2Vec model on your tokenized text data. You can use the `Word2Vec` class from the `gensim` library."
      ],
      "metadata": {
        "id": "bHFU1z-e9a5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=[normalized_tokens], vector_size=100, window=5, min_count=1, sg=0)\n"
      ],
      "metadata": {
        "id": "BHlKOzal4B0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Get Word Vectors:\n",
        "Once you have the trained Word2Vec model, you can obtain the dense vectors for words."
      ],
      "metadata": {
        "id": "ERGuo07_-DAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get vector for a specific word\n",
        "word_vector = w2v_model.wv['म']\n",
        "\n",
        "# Get vectors for a list of words\n",
        "word_vectors = [w2v_model.wv[word] for word in normalized_tokens]\n"
      ],
      "metadata": {
        "id": "4k8M0FLV9oUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Padding Word Vectors:\n",
        "If you want to maintain a fixed sequence length, you'll need to pad the word vectors similar to what you did with the integer indices in the previous example."
      ],
      "metadata": {
        "id": "qicRuusE_a9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_word2vec = [word_vectors[i:i+sequence_length] for i in range(len(word_vectors) - sequence_length)]\n",
        "X_word2vec_padded = pad_sequences(X_word2vec, maxlen=sequence_length, padding='post')\n"
      ],
      "metadata": {
        "id": "33oKyds9-dxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have the word vectors from the Word2Vec model represented as dense vectors ready for training."
      ],
      "metadata": {
        "id": "ckd9uAASACqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Data Splitting:\n",
        "\n",
        "divide dataset into a training set and a test set using the common 80-20 split.\n",
        "\n",
        "training the model on the training set, it evaluates the model's performance on the test set using the evaluate method."
      ],
      "metadata": {
        "id": "G5JOPD_bC3XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_padded, y_encoded, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "TfXmey9oC2pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Build and Train LSTM Model:\n",
        "Next, you can build an LSTM model to predict part-of-speech tags using the word vectors as input."
      ],
      "metadata": {
        "id": "sAsmXoOMAFbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.models import Sequential\n",
        "# from keras.layers import LSTM, Dense\n",
        "\n",
        "# # Build LSTM model\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(units=128, input_shape=(sequence_length, 100)))  # Assuming vector_size=100 in Word2Vec\n",
        "# model.add(Dense(len(tag_set), activation='softmax'))\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(X_word2vec_padded, y_encoded, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "JeHjBQVT_jV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds and trains an LSTM model using the Word2Vec word vectors as input. You might need to adjust hyperparameters, such as the number of LSTM units, the vector size in the Word2Vec model, and the training parameters, according to your specific problem and dataset."
      ],
      "metadata": {
        "id": "H3903wzlAbbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=128, input_shape=(sequence_length, 100)))  # Assuming vector_size=100 in Word2Vec\n",
        "model.add(Dense(len(tag_set), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3PEmkSOANOm",
        "outputId": "44e45a9b-02e5-42e3-fbc4-475a1f4ded41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "908/908 [==============================] - 15s 15ms/step - loss: 0.7239 - accuracy: 0.8738 - val_loss: 0.6688 - val_accuracy: 0.8768\n",
            "Epoch 2/10\n",
            "908/908 [==============================] - 13s 14ms/step - loss: 0.6718 - accuracy: 0.8748 - val_loss: 0.6649 - val_accuracy: 0.8768\n",
            "Epoch 3/10\n",
            "908/908 [==============================] - 13s 14ms/step - loss: 0.6710 - accuracy: 0.8748 - val_loss: 0.6662 - val_accuracy: 0.8768\n",
            "Epoch 4/10\n",
            "908/908 [==============================] - 14s 15ms/step - loss: 0.6703 - accuracy: 0.8748 - val_loss: 0.6611 - val_accuracy: 0.8768\n",
            "Epoch 5/10\n",
            "908/908 [==============================] - 14s 16ms/step - loss: 0.6698 - accuracy: 0.8748 - val_loss: 0.6594 - val_accuracy: 0.8768\n",
            "Epoch 6/10\n",
            "908/908 [==============================] - 13s 14ms/step - loss: 0.6683 - accuracy: 0.8748 - val_loss: 0.6592 - val_accuracy: 0.8768\n",
            "Epoch 7/10\n",
            "908/908 [==============================] - 13s 14ms/step - loss: 0.6656 - accuracy: 0.8748 - val_loss: 0.6626 - val_accuracy: 0.8768\n",
            "Epoch 8/10\n",
            "908/908 [==============================] - 13s 14ms/step - loss: 0.6647 - accuracy: 0.8748 - val_loss: 0.6587 - val_accuracy: 0.8768\n",
            "Epoch 9/10\n",
            "908/908 [==============================] - 13s 14ms/step - loss: 0.6632 - accuracy: 0.8748 - val_loss: 0.6582 - val_accuracy: 0.8768\n",
            "Epoch 10/10\n",
            "908/908 [==============================] - 13s 15ms/step - loss: 0.6618 - accuracy: 0.8748 - val_loss: 0.6591 - val_accuracy: 0.8768\n",
            "284/284 [==============================] - 1s 5ms/step - loss: 0.6415 - accuracy: 0.8794\n",
            "Test loss: 0.6415, Test accuracy: 0.8794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All the above step are together\n"
      ],
      "metadata": {
        "id": "7g228_tIHpEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hrar4uuerYD",
        "outputId": "03cdd138-6efa-4ffb-d56d-9d3165d2e591"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk gensim scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz4a3RmFfBU7",
        "outputId": "929e7855-7bb4-4604-9b2e-f75c0b3373d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow nltk gensim scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-lhdZoZnfHJJ",
        "outputId": "19188f3c-f86d-4cbe-d0a7-11e491d4d201"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, keras, scikit-learn, tensorboard, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic 2.1.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.4.0 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.13.1 scikit-learn-1.3.0 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "sklearn",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "# Load and preprocess your XML data\n",
        "### Load the XML data\n",
        "tree = ET.parse('book-academic-jana-siksha-2058.xml')\n",
        "root = tree.getroot()\n",
        "\n",
        "### Extract text content from <w> tags\n",
        "text_data = ' '.join([w.text for w in root.iter('w')])\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize and normalize the text data\n",
        "### Tokenize the text data\n",
        "tokens = word_tokenize(text_data)\n",
        "\n",
        "### remove special tags\n",
        "tokens = [token for token in tokens if not token.startswith('<')]\n",
        "\n",
        "### Text normalization\n",
        "normalized_tokens = [token.lower() for token in tokens]\n",
        "\n",
        "# Create vocabulary and index mapping\n",
        "vocab = set(normalized_tokens)\n",
        "word_to_index = {word: index + 1 for index, word in enumerate(vocab)}  # Start index from 1\n",
        "index_to_word = {index: word for word, index in word_to_index.items()}\n",
        "\n",
        "# Convert words to indices\n",
        "indexed_data = [word_to_index[word] for word in normalized_tokens]\n",
        "\n",
        "# Define the sequence length for LSTM input\n",
        "sequence_length = 10\n",
        "\n",
        "# Step 2: Word Embedding using Word2Vec\n",
        "w2v_model = Word2Vec(sentences=[normalized_tokens], vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "# Get word vectors\n",
        "### Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=[normalized_tokens], vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "### Get vector for a specific word\n",
        "word_vector = w2v_model.wv['म']\n",
        "\n",
        "### Get vectors for a list of words\n",
        "word_vectors = [w2v_model.wv[word] for word in normalized_tokens]\n",
        "\n",
        "# Pad word vectors\n",
        "X_word2vec = [word_vectors[i:i+sequence_length] for i in range(len(word_vectors) - sequence_length)]\n",
        "X_word2vec_padded = pad_sequences(X_word2vec, maxlen=sequence_length, padding='post')\n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Encoding Labels (e.g., one-hot encoding or label encoding):\n",
        "### Perform part-of-speech tagging on the original text\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "### Create a mapping from part-of-speech tags to unique indices\n",
        "tag_set = set(tag for word, tag in pos_tags)\n",
        "tag_to_index = {tag: index for index, tag in enumerate(tag_set)}\n",
        "\n",
        "### generate training example\n",
        "y = [tag_to_index[pos_tags[i + sequence_length][1]] for i in range(len(indexed_data) - sequence_length)]\n",
        "\n",
        "# Encoding Labels (e.g., one-hot encoding or label encoding):\n",
        "y_encoded = np.eye(len(tag_set))[y]\n",
        "# -------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Step 3: Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_padded, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Create LSTM Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=128, input_shape=(sequence_length, 100)))\n",
        "model.add(Dense(len(tag_set), activation='softmax'))\n",
        "\n",
        "# Step 5: Compile Model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 6: Train Model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Step 7: Evaluate Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nLa1A-vD4zx",
        "outputId": "694a9fa8-c6d5-4a4d-e4da-67818015a08a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "908/908 [==============================] - 28s 28ms/step - loss: 0.7219 - accuracy: 0.8735 - val_loss: 0.6634 - val_accuracy: 0.8768\n",
            "Epoch 2/10\n",
            "908/908 [==============================] - 22s 25ms/step - loss: 0.6723 - accuracy: 0.8748 - val_loss: 0.6657 - val_accuracy: 0.8768\n",
            "Epoch 3/10\n",
            "908/908 [==============================] - 22s 24ms/step - loss: 0.6715 - accuracy: 0.8748 - val_loss: 0.6649 - val_accuracy: 0.8768\n",
            "Epoch 4/10\n",
            "908/908 [==============================] - 30s 33ms/step - loss: 0.6707 - accuracy: 0.8748 - val_loss: 0.6671 - val_accuracy: 0.8768\n",
            "Epoch 5/10\n",
            "908/908 [==============================] - 22s 24ms/step - loss: 0.6700 - accuracy: 0.8748 - val_loss: 0.6637 - val_accuracy: 0.8768\n",
            "Epoch 6/10\n",
            "908/908 [==============================] - 29s 32ms/step - loss: 0.6689 - accuracy: 0.8748 - val_loss: 0.6585 - val_accuracy: 0.8768\n",
            "Epoch 7/10\n",
            "908/908 [==============================] - 22s 24ms/step - loss: 0.6672 - accuracy: 0.8748 - val_loss: 0.6585 - val_accuracy: 0.8768\n",
            "Epoch 8/10\n",
            "908/908 [==============================] - 25s 28ms/step - loss: 0.6652 - accuracy: 0.8748 - val_loss: 0.6611 - val_accuracy: 0.8768\n",
            "Epoch 9/10\n",
            "908/908 [==============================] - 22s 24ms/step - loss: 0.6638 - accuracy: 0.8748 - val_loss: 0.6588 - val_accuracy: 0.8768\n",
            "Epoch 10/10\n",
            "908/908 [==============================] - 22s 24ms/step - loss: 0.6625 - accuracy: 0.8748 - val_loss: 0.6584 - val_accuracy: 0.8768\n",
            "284/284 [==============================] - 2s 7ms/step - loss: 0.6394 - accuracy: 0.8794\n",
            "Test loss: 0.6394, Test accuracy: 0.8794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HjSrgpZII-w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}